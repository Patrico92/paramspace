\section{A Case Study: VLIW Hardware/Software Design Space}
\label{sec:ee}
\subsection{Motivation}
In order to evaluate the behavior of the proposed approach
we will be using a parameterized Very Long Instruction
Word (VLIW) architecture~\cite{kathail_tr00} as testbed. The same philosophy
behind a VLIW system makes this a natural choice for several reasons:
\begin{itemize}
\item \emph{Multi-Objective trade-offs}: VLIW architectures let designers
easily trade between power, energy and performances, resulting
in extended Pareto sets which are an ideal ground for our testing
purposes focused on parameter-space representation
\item \emph{Compiler awareness}: code scheduling for the execution of
applications is statically obtained by the compiler which
performs code transformations ``being aware'' of the underlying hardware
configuration.  This tight hardware/software dependence makes the
VLIW scenario perfectly suitable for investigating hardly predictable
parameter interactions.
\end{itemize}

\subsection{Parameter Space}
In general, system parameters can be classified in three main categories:
\emph{processor}, \emph{memory sub-system} and \emph{compiler}. 
The first is directly related to the VLIW core and includes the size
of the register files and the number of functional
units for each type. In particular, five different types of register
files are supported:
GPR (32-bit registers for integers), FPR (64-bit registers for
floating point values) PR (1-bit registers used to store the Boolean
values of predicated instructions), BTR (64-bit registers containing
information about possible future branches) and CR (32-bit control
registers containing information about the internal state of the
processor). The functional units involved are: \emph{Integer units},
\emph{floating point units}, \emph{memory units} (associated with
load/store operations) and \emph{branch units} (associated with branch
operations). With respect to the memory sub-system, the parameters
include \emph{size}, \emph{associativity} and
\emph{block size} for each of the three caches: First-level data cache
(L1D), first-level instruction cache (L1I) and second-level unified
cache (L2U).

Next, a set of the most impacting code compilation parameters has been
selected and included in the configuration space:
\begin{itemize}
\item \emph{tcc\_region}: Specifies the scope of action of the compiler and the type
of code transformation involved (basic block, super block and hyper
block) 
\item \emph {max\_unroll\_allowed}: The number of unroll iterations allowed
\item \emph{regroup\_only}: Avoids inlining 
\item \emph{do\_classic\_opti}: A set of classical optimization, not VLIW related,
such as common expression removal 
\item \emph{do\_prepass\_scalar\_scheduling}: Performs a schedule before
forming regions 
\item \emph{do\_postpass\_scalar\_scheduling}: Performs a schedule after the region formation 
do\_modulo\_scheduling  Modular scheduling 
\item \emph{memvr\_profiled}: Performs a memory-dependencies profiling 
\end{itemize}

As regards the class of benchmark being considered, a set applications representative of some common
frequently running code kernels has been selected. Table~\ref{tab:bench} shows the set of applications
along with a brief description.
% IEEE table style
%\begin{table}
%	\centering
%	\caption{Benchmarks}
%	\label{tab:bench}
%	\begin{tabular}{ll}
%	\hline
%	\multicolumn{1}{c}{Benchmark} & \multicolumn{1}{c}{Application} \\
%	\hline
%	Alloca\_test & Memory array allocation test \\
%	Bmm & Matrices multiplication and elements sum \\
%	Fir\_int & Finite impulse response \\
%	Mm & Floating point matrices multiplication \\
%	Sha256 & Cryptocurrency header Hashing \\
%	Wave & Wavefront computation \\
%	\hline
%	\end{tabular}
%\end{table}

% ACM table style
\begin{table}
	\tbl{Benchmarks\label{tab:bench}}{
	\begin{tabular}{ll}
	\hline
	\multicolumn{1}{c}{Benchmark} & \multicolumn{1}{c}{Application} \\
	\hline
	Alloca\_test & Memory array allocation test \\
	Bmm & Matrices multiplication and elements sum \\
	Fir\_int & Finite impulse response \\
	Mm & Floating point matrices multiplication \\
	Sha256 & Cryptocurrency header Hashing \\
	Wave & Wavefront computation \\
	\hline
	\end{tabular}}
\end{table}
\subsection{Evaluation Metrics}

The results presented in the next section compare PS one widely
adopted Multi-objective Genetic Algorithm
(MOGA)~\cite{knowles_techrep06}. At best of our knowledge, genetic
approaches in general still represent the best compromise between
efficiency (i.e. time required for exploration) and accuracy of the
reported Paret Sets.  Note that for the reasons described above,
mono-objective approaches have been explicitly excluded from this
analysis. Same decision has been taken for other approaches (e.g.
dependency based~\cite{givargis_tvlsi02}) that make use of some
a-priori knowledge about the role and meaning of each parameter, since
our aim is evaluate the capacity of focusing on interesting parameter
regions without any external judgement or help based on heuristics.
Finally, it should be pointed out that in this introductory work we
will explicitly use the \emph{SPEA}~\cite{zitzler_eurogen01} variant of MOGA,
remanding to works like~\cite{zitzler_ec00}~\cite{zitzler_tec03} for a
detailed analysis on how it differs and what could be expected from
similar multi-objective strategies.

%------------------------------------------------------------------------------

\subsection{Results}

Set of Figures~\ref{fig:pareto_fronts_100}
and~\ref{fig:pareto_fronts_500} show the resulting pareto fronts for two
different scenarios of 100 and 500 simulations budget. When possible,
similar algorithm parameters have been adopted. In particular a
population size of 30 has been chosen for the genetic approach (GA), corresponding to the
same value for the $K$ of PS. The number of generations (or era)
has been chosen so that the only the given amount of simulations is
actually performed. Thus, when accounting these simulations only unique simulations
has been considered, removing repetitions and unfeasible
configurations.

% IEEE table style
%\begin{table}
%  \centering
%  \begin{tabular}{ccc}
%    \includegraphics[width=0.30\textwidth]{pictures/alloca_100.eps} &
%    \includegraphics[width=0.30\textwidth]{pictures/bmm_100.eps} & 
%    \includegraphics[width=0.30\textwidth]{pictures/fir_int100.eps} \\
%    \includegraphics[width=0.30\textwidth]{pictures/mm_100.eps} &
%    \includegraphics[width=0.30\textwidth]{pictures/sha_100.eps} &
%    \includegraphics[width=0.30\textwidth]{pictures/wave_100.eps} 
%  \end{tabular}
%  \caption{Pareto fronts found by PS and GA for a fixed budget of 100 configurations.}
%  \label{fig:pareto_fronts_100}
%\end{table}
%
%\begin{table}
%  \centering
%  \begin{tabular}{ccc}
%    \includegraphics[width=0.30\textwidth]{pictures/alloca_500.eps} &
%    \includegraphics[width=0.30\textwidth]{pictures/bmm_500.eps} & 
%    \includegraphics[width=0.30\textwidth]{pictures/fir_int500.eps} \\
%    \includegraphics[width=0.30\textwidth]{pictures/mm_500.eps} &
%    \includegraphics[width=0.30\textwidth]{pictures/sha_500.eps} &
%    \includegraphics[width=0.30\textwidth]{pictures/wave_500.eps} 
%  \end{tabular}
%  \caption{Pareto fronts found by PS and GA for a fixed budget of 500 configurations.}
%  \label{fig:pareto_fronts_500}
%\end{table}

\begin{table}
  \tbl{Pareto fronts found by PS and GA for a fixed budget of 100 configurations.\label{fig:pareto_fronts_100}}{
  \begin{tabular}{ccc}
    \includegraphics[width=0.30\textwidth]{pictures/alloca_100.eps} &
    \includegraphics[width=0.30\textwidth]{pictures/bmm_100.eps} & 
    \includegraphics[width=0.30\textwidth]{pictures/fir_int100.eps} \\
    \includegraphics[width=0.30\textwidth]{pictures/mm_100.eps} &
    \includegraphics[width=0.30\textwidth]{pictures/sha_100.eps} &
    \includegraphics[width=0.30\textwidth]{pictures/wave_100.eps} 
  \end{tabular}}
  
\end{table}

\begin{table}
  \tbl{Pareto fronts found by PS and GA for a fixed budget of 500
  configurations.\label{fig:pareto_fronts_500}}{
  \begin{tabular}{ccc}
    \includegraphics[width=0.30\textwidth]{pictures/alloca_500.eps} &
    \includegraphics[width=0.30\textwidth]{pictures/bmm_500.eps} & 
    \includegraphics[width=0.30\textwidth]{pictures/fir_int500.eps} \\
    \includegraphics[width=0.30\textwidth]{pictures/mm_500.eps} &
    \includegraphics[width=0.30\textwidth]{pictures/sha_500.eps} &
    \includegraphics[width=0.30\textwidth]{pictures/wave_500.eps} 
  \end{tabular}}
\end{table}
Low budget Pareto sets are shown in
Figure~\ref{fig:pareto_fronts_100}. With a so small number of
configuration being investigated the two sets are often overlapping,
being also strongly depended upon the application being considered.
These results are obtained in about 5 eras, and the main aim was
demonstrating that with a so limited budget PS still can compete with
genetic approach while even discovering very different Pareto fronts.
Considering the extended simulation budget as depicted in
Figure~\ref{fig:pareto_fronts_500}, we can qualitatively observe that
even if PS does not strictly outperforms genetic in terms of dominance, it shows in
some cases largely Pareto in terms of extension. We can intuitively
relate this to the ``novelty-based'' score system, so that instead of
focusing on getting better points inside a given range it tries to
enlarge the range itself, resulting in less dense but more extended
Paretos. Of course we cannot classify this different behaviour as
``better'' or ``worse'' as compared to the genetic one, it depends on
various design factors such as desired results granularity, prefixed objective
range constraints, estimated error of the models used etc~\ldots.

In order to evaluate the Paretos from a quantitative point-of-view we
will consider two metrics, namely, the \emph{variation range} and the
\emph{average normalised absolute dispersion error}. For a given
objective, the variation range represents the ratio between the
maximum and the minimum value observed for that objective. A
comparison between the variation range for different benchmarks
between a PS and a GA exploration for both power dissipation and
execution time is shown in Fig.~\ref{fig:dispersion}(a) and (b)
respectively. As it can be observed, the PS exploration provides solutions which fall on a
range that is, on average, wider than that provided by a
GA exploration for power dissipation and execution time, respectively.
%\begin{table}
%  \centering
%  \begin{tabular}{c}
%    \includegraphics[width=0.7\textwidth]{pictures/variation_power.eps} \\
%    (a) \\
%    \includegraphics[width=0.7\textwidth]{pictures/variation_etime.eps} \\
%    (b) \\
%    \includegraphics[width=0.7\textwidth]{pictures/dispersion.eps} \\
%    (c) 
%  \end{tabular}
%  \caption{(a)(b) Variation range for different benchmarks between PS and GA
%  and average dispersion absolute dispersion (c)}
%  \label{fig:dispersion}
%\end{table}

\begin{table}
  \tbl{(a)(b) Variation range for different benchmarks between PS and GA
  and average dispersion absolute dispersion (c)
  \label{fig:dispersion}}{
  \begin{tabular}{c}
    \includegraphics[width=0.7\textwidth]{pictures/variation_power.eps} \\
    (a) \\
    \includegraphics[width=0.7\textwidth]{pictures/variation_etime.eps} \\
    (b) \\
    \includegraphics[width=0.7\textwidth]{pictures/dispersion.eps} \\
    (c) 
  \end{tabular}}
\end{table}
The average normalised absolute dispersion error measures
the average absolute difference between the distribution of points in
the objective space and an ideal distribution in which the points are
uniformly distributed over the objective space. Formally, let $O$ be
the image, in the objective space, of the configurations visited by
the design space exploration. The generic element of $O$ (\ie, a
solution) is a pair $(p,t)$ where $p$ and $t$ are the average power
and execution time, respectively. The two-dimensional objective space
is then partitioned by a $M_x \times M_y$ mesh. For each tile $T_i,
\ i=1, 2, \ldots, M_xM_y$ of the mesh, let $N_i$ be the number
of points in $O$ which fall in $T_i$. The average absolute error, $E_i$, for
$T_i$ is the absolute value of the difference between $N_i$ and the
ideal number of solutions, $\overline{N}$, which should fall in $T_i$
in case of uniform distribution. Such $\overline{N}$ can be simply
computed as the ratio between the cardinality of $O$ and the number of
tiles. Thus,
\[ E_i = |N_i - \overline{N}|, \]
where $\overline{N} = |O| / (M_x M_y)$. The average
normalised absolute dispersion error ($ANADE$) is the average of $E_i$
normalised to the maximum absolute error $E_{max}$:
\[ ANADE = \frac{\sum_{i=1}^{M_xM_y} E_i/(M_xM_y)}{E_{max}}, \]
where $E_{max}$ can be computed as the average absolute error in the worst
case in which all the solutions fall in a single tile:
\[ E_{max} = \frac{(M_x M_y - 1) \overline{N} + |\overline{N} -
    |O|| }{M_x M_y}. \] 

\comment{DP:il discorso si concludeva troppo di botto, ho aggiunto una
specie di finale che devi immaginare tipo il finale di rocky ''se io posso cambiare, e tu puoi
cambiare, tutto il mondo puo' cambiare, adriaaaanaa! ''}
Fig.~\ref{fig:dispersion}(c) shows the average normalised absolute
dispersion errors for different benchmarks for PS and GA
exploration. As it can be observed, GA still shows a tendency in
discovering more uniformly dispersed points (i.e. higher values in the
chart). These results,
together with the variation ranges of Fig.~\ref{fig:dispersion}(a) and
(b) confirm the the availability of density vs extension trade-offs
when comparing PS and GA approaches. Again, it
should be pointed out how a definitive claim about the superiority of
the PS behaviour is stricly dependent on the design 
requirements and constraints, e.g. whether we need wide or dense
results. Nevetheless, the proposed algorithm introduces a new
perspective on thinking the allocation of a simulations budget, paving the way for
the development of further strategies for addressing the problem of
multi-objective hardware/software desing space exploration.


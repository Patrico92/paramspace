
\section{Introduction}

The design of an embedded system requires several objectives (energy
consumption, area occupation, ...) to be fulfilled. The fulfilling
of these objectives depends on several parameters and in the real
cases no analytical results are known to show the relation between
configurations (combination of parameters) and objectives. Therefore
the simulation is the only way to have a picture of how parameters
impact on the objectives. The problem is that the number of parameters
and the number of values that each of them may take lead to an enormous
number of possible configurations, such that simulating all of them
is an unfeasible activity. Therefore there is the need for the ``Design
Space Exploration'', i.e. a methodology to select a subset of all
possible configurations and simulate only this subset.

Many different design space exploration algorithms have been proposed.
A design space exploration algorithm is typically not based on precise
/ analytical / mathematical evidences. The motivations of an exploration
algorithm are rather heuristic, have some form of arbitrariness and,
to a large extend, intuition lies behind them.



The result of every design space exploration is a subset of the configurations
called Pareto Set (\cite{pareto}). Roughly speaking, it is a set of
configurations such that there are no other configurations, among
the simulated ones, that have all the objectives, at the same time,
better fulfilled. If all possible configurations were evaluated, the
ideal Pareto set (the ``true'' Pareto set) could be calculated.
But, as stated above, evaluating all the configurations is not feasable.
Therefore the Pareto set returned by an exploration algorithm is an
approximation of the ``true'' Pareto set.

Many exploration algorithms form the resulting Pareto set in an incremental
way. The algorithm adds and removes configurations in Pareto set while
choosing the configurations to evaluate and simulating them. So Pareto
set ``evolves'' towards the final one. The dynamical view of the
Pareto set shaping can be seen as the ``behavior'' of the algorithm.
This behavior tells us how the Pareto set ``converges'' towards
the final Pareto set. 

The ``quality'' of an exploration algorithm involves two aspects:
\begin{itemize}
\item how much the Pareto set returned by the exploration algorithm is similar
to the ``true'' Pareto set (notice that this cannot be evaluated
because the true Pareto set is unknown - it is only an ``ideal''
quality factor)
\item how fast the Pareto set converges towards the final one. If algorithm
$1$ returns the final Pareto set after simulating $n_{1}$ configurations
and algorithm $2$ returns its final Pareto set after $n_{2}$ simulations,
algorithm $1$ is said to be faster than algorithm $2$, iff $n_{1}<n_{2}$.
\end{itemize}

\section{\label{sec:Related-work}Related work}

There are a number of design space exploration algorithms proposed
in literature.

The ``Dependency analysis'' proposed in \cite{givargis_tvlsi02}, is meant
to take advantage of the dependency among the parameters in a multiobjective
optimization problem. It is required that the designer knows, prior
to the exploration, if a parameter X is dependent from a parameter
Y, i.e. changing the value of Y ``impacts the optimal parameter value''
for X. With this a-priori knowledge, the designer can construct a
``dependency graph'' and recognize clusters, i.e. subsets of strongly
dependency-connected parameters. Each cluster is exhaustively evaluated
and its ``local Pareto set'' is found. Then, all local Pareto sets
are merged. Doing this way, a ``sum'' of local exhaustive evaluations
are performed instead of an exhaustive evaluation of all the possible
configurations. Some problems arise: i) In real scenarios, it may
be very difficult to recognize really independent clusters of parameters,
because there may be interdependencies among a very large number of
parameters. This may lead to the need of an exhaustive search through
almost all the possible configurations. ii) A designer may not have
a precise and complete picture of the dependencies among parameters;
for this reason the need of ``automated approaches for computing
{[}...{]} interdependencies'' is declared in the same paper. iii)
There may be some ``local dependencies'', i.e. dependencies that
emerge only if parameter values are bounded to certain ranges.

These drawbacks are not present in our solution. The algorithm presented
here is also meant to take advantage of dependency but is not directly
based on dependency itself, but rather it ``catches'' dependencies
in terms of ``interesting or uninteresting regions''; as a consequence
no a-priori knowledge of dependencies is required as they are discovered
during exploration. Moreover, recognizing regions offers the capability
to capture local dependencies.

Abraham, Rau and Schreiber proposed in \cite{santosh_hptr00} to decompose
the system to be evaluated into components that interact minimally
with each other. ``Alternative designs for each component are evaluated
in isolation to determine the best designs for individual components.
Combinations of these designs are then put together to build the complete
system design''. Pareto sets for each component are found and, provided
that ``monotonicity'' exists, the complete Pareto set is computed
merging the component Pareto sets. Roughly speaking (see section 4
of the same paper for more details), monotonicity property guarantees
that the best system can be obtained as a composition of the best
components. This approach would perfectly work if all the components
were perfectly isolated, i.e., if there were no dependencies among
different components (in a sense similar (but not necesarily equal)
to the one of Dependency analisys). But real scenarios seldom if ever
expose monotonicity property. This leads to some ``inaccuracies''
(as stated in section 4.6.1 of the same paper).

Other approaches take into account the concept of ``sensitivity''.
The sensitivity of an objective with the respect to a parameter is
a measure of how much the objective varies when varying that parameter.
A ``sensitivity analysis'' was used in \cite{fornaciari_codes01} in an optimization
problem with only one objective (the power-delay product of an electronic
device). This method was extended in \cite{palesi_iwsoc02} to a multiobjective
optimization. In that work the sensitivity of objectives with
the respect to each parameter is measured, and parameters are ordered
on the base on these sensitivity measures. Then, a first Pareto set
is obtained varying the first parameter, having fixed to arbitrary
values all other parameters. The Pareto set is then refined varying,
in a similar way, the other parameters, one by one. Experimental results
(see section 4.2 of the same work) show that this method is not of
good ``quality'' (as broadly defined in the introduction). The reason
can be found in the very limited and rigid exploration of the parameter
space: after computing the first Pareto set, there are no more chances
to consider configurations with values of the first parameter other
than the ones assumed in the first Pareto set calculation. Similar
limitations are imposed after computing the second Pareto set, the
third and so on. It is worth noticing that this approach can not capture
``local sensitivity'' (this drawback will be called ``local sensitivity
limitation''), i.e. the objectives may be more sensitive to some
ranges of a parameter and less sensitive to other ranges of the same
parameter. Moreover it can not capture ``combined sensitivity''
(``combined sensitivity limitation''), i.e. the objectives may be
more sensitive to a range of a parameter $p_{1}$, only when other
parameters are within certain ranges, and less sensitive to the same
range of parameter $p_{1}$ when the other parameters are not within
those ranges. Therefore, in the method we propose, we do not use the
sensitivity directly, but we use the concept of innovation (that can
be considered, at least weakly related to the sensitivity). We do
not measure innovation of a parameter (as done for the sensitivity),
but rather we measure the innovation of a ``region'' of parameter
space, in order to overcome both the ``local sensitivity limitation''
and the ``combined sensitivity limitation'' stated above for the
sensitivity analysis.

Many studies focus on genetic approaches to resolve multiobjective
optimization problems (\cite{} and others). Genetic approaches
have many advantages: they have proved good quality (as broadly defined
in the introduction), they are customizable, they are very general
and require no a-priori knowledge to the designer. Genetic algorithms
run in subsequent iterations. Configurations are represented as chromosomes.
In each iteration a ``population'' of configurations is evaluated.
Some configurations are discarded. Each selected configuration is
combined with the preceding population and then ``mutated'' (i.e.
the resulting configuration is varied a little). Iterating this operations,
populations are expected to become ``better and better''. The strong
point of genetic approaches can be summarized saying that they consist
in a broad exploration (the mutation permits to make the exploration
not rigidly restricted to limited parts of parameter space), although
this exploration is not random but is guided by the performances of
the already evaluated configurations. Therefore, the exploration is
neither too much constrained nor too much random. We claim that the
approach presented in this paper has all the benefits of the genetic
approaches, although the rational is completely different.

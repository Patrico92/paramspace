
\section{Introduction and Motivation}

The design of an embedded system requires different conflicting
objectives (energy consumption, performance, area) to be fulfilled.
Several factors are involved in this goal and real cases show how
often no analytical results are known to predict the relation between
system configurations and objectives. In such cases a high level
system simulation is the only way to have a picture of how system
parameters impact on the objectives.  A key problem is that the size
of the design space to be simulated grows with the product of
cardinalities of each parameter, resulting in an intractable number
of simulations. In these cases a Design Space
Exploration (DSE) strategy is required, i.e. an algorithm that selects
only a tractable subset of all possible configurations to simulate while providing, at the end, configurations that are close to the optimum and that are worth evaluating, in a following step of the design process, in a more thorough way.

%The final result of a design space exploration is a subset of the configurations called \emph{Pareto set} (\cite{pareto}, see also \defR{pareto-set}). The corresponding values of the objectives for each configuration of the Pareto Set is called \emph{Pareto front}: it represents the trade-offs between objectives, i.e.  for each point of the Pareto front there is no other configuration performing better for all the objectives considered. Once these few promising configurations have been obtained, a subsequent step of the design flow can eventually afford a more accurate low level simulation.

In this work we present a multiobjective design exploration algorithm
that introduces the concept of interesting and uninteresting regions of the configuration space.
\comment{AA: prima era ``algorithm
that introduces the concept of Parameter Space Representation of
Pareto front''. Se io fossi un reviewer, direi: guardi, questo io non credo, anche pecche' non siamo noi che introduciamo il concetto di parameter space representation del pareto set. Il concetto esiste gia' dal 1896 e si chiama ``Pareto-set''! Ho cambiato il claim dicendo che introduciamo interesting/uninteresting regions, che mi sembra piu' rispondente alla verita'. Cosi' ti va bene?} 
The algorithm is iterative. The main idea can be simplified as follows. At each iteration, the configuration space is divided in regions. A score is given to each region based on the \emph{innovation} provided by its configurations in the Pareto front w.r.t. the previous iterations. The regions with high score are considered the most \emph{interesting} and more simulations are distributed there rather than in the rest of the configuration space. Since the fundamental operations consist in the formation of regions in the Parameter Space (aka configuration space), we call our algorithm PS.

\comment{AA: Le seguenti referenze a cose di psicologia cognitiva potrebbero essere interpretate come una figata o una cagata. Tu come le interpreti?}
Our algorithm mimics the attention shift in human beings. Authors in \cite{attention} models human attention as a single resource with a limited budget that must be distributed among tasks, preferring the task which the focus is on.
 Authors in \cite{spatial_attention} describes the visual attention as a spotlight that focuses, each time, in a particular region of the visual space in order to process it more deeply. Similarly, PS focuses its ``spotlight'' only to the interesting regions and explore them more thoroughly distributing the simulation budget on them.
% Ispirzione: http://en.wikipedia.org/wiki/Attentional_shift

With PS, we propose a new
perspective on the configuration space, where parameter
interdependencies play the fundamental role of making some regions
more capable of generating hardly predictable solutions: catching this
sort of ``chaoticity'' in the parameter space is the main idea behind
the strategy that is being introduced in this work.


The paper is organized as follows. First, \secR{Related-work} places our work in the context of related effort.
Then we give a theoretical formulation of the algorithm. In particular, in \secR{statement_of_the_problem} we provide formal definitions of the concepts and the operations that will be used to describe PS algorithm. In \secR{algorithm} we give the theoretical description of PS. 
Finally in Section~\ref{sec:ee} we show a case
study involving the exploration of the hardware/software parameters of
a Very Long Instruction Word (VLIW) architecture, performing a
qualitative and quantitative comparison of PS against the state-of-art
of multiobjective genetic based approaches.


\section{Related work}
\secL{Related-work}
Many different design space exploration algorithms have been proposed
in literature.  The motivations of an exploration
algorithm are rather heuristic, have some form of arbitrariness and,
to a large extend, intuition lies behind them.

Some exploration strategies assume some kind of knowledge about the
system parameters, their meaning and their impact on design
objectives.  The \emph{Dependency analysis}, proposed in
\cite{givargis_tvlsi02}, is meant to take advantage of the parameter
dependencies. Starting from them, he can construct a ``dependency graph''
and recognize clusters, i.e. subsets of strongly dependency-connected
parameters. Each cluster is exhaustively evaluated and its ``local''
Pareto set is found. Then, all local Pareto sets are merged. In this
way, a series of local exhaustive evaluations are performed instead of
an exhaustive evaluation of all the possible configurations. Some
problems arise: i) In real scenarios, it may be difficult to recognize
really independent clusters of parameters, because there may be
interdependencies among a large number of parameters. This may lead to
the need of an evaluation of almost all the possible configurations.
ii) A designer may not have a precise and complete a-priori picture of
the dependencies among parameters; for this reason the need of
``automated approaches for computing interdependencies'' is declared
in the same paper.  These drawbacks are not present in our algorithm,
since, although it leverages dependencies as in
\cite{givargis_tvlsi02}, it is able to detect them automatically, with
no a-priori knowledge required. Moreover, our algorithm is able to
capture also ``local dependencies", i.e. dependencies that emerge only
among certain ranges of parameters and may not hold when considering
the entires ranges. This cannot be modelled in Dependency Analysis. 

Abraham, Rau and Schreiber proposed in \cite{santosh_hptr00} to decompose
the system under evaluation into components that interact minimally
with each other. Pareto sets for each component are found and, provided
that ``monotonicity'' exists, the complete Pareto set is computed
merging the component Pareto sets. Roughly speaking (see section 4
of the same paper for more details), monotonicity property guarantees
that the best system can be obtained as a composition of the best
components. This approach would perfectly work if all the components
were perfectly isolated, i.e., if there were no dependencies among
them. But real scenarios seldom if ever
expose monotonicity property thus resulting in inaccuracies, as stated in the same paper).

Other approaches, as \cite{fornaciari_codes01,palesi_iwsoc02}, are based the concept of \emph{sensitivity analysis}, i.e. measuring of how much the objective varies when varying each parameter.
Parameters are ordered based on their sensitivity. The first parameter (the most sensitive one) is varied, while keeping the other parameters fixed to arbitrary values, and its best value is found. The limited accuracy shown by these approaches can be explained by the limited and rigid exploration of the parameter
space: after fixing the best first parameter value, there are no more chances to consider different values. It is worth noticing that this approach can not capture
``local sensitivity'', i.e. the objectives may be more sensitive to some
ranges of a parameter and less sensitive to other ranges of the same
parameter. Moreover it can not capture ``combined sensitivity'', i.e. the objectives may be
more sensitive to a range of a parameter $p_{1}$, only when other
parameters are within certain ranges, and less sensitive to the same
range of $p_{1}$ when the other parameters are in different ranges. Otherwise, our algorithm can both capture local and combined sensitivity.


Many studies, as \cite{coello_easmop} and others, focus on genetic approaches to solve multiobjective
optimization problems. Genetic approaches
have many advantages: they provide good accuracy, they are customizable and very general
and require no a-priori knowledge to the designer.  The strong
point of genetic approaches can be summarized saying that they consist
in exploration that is sufficiently broad (the mutation avoids being rigidly restricted to limited parts of parameter space), and, at the same time, not too scattered as it is guided by the performances of
the already evaluated configurations. In this work we will compare 
the proposed approach against a well known Multi-Objective Genetic
Algoritm~\cite{knowles_techrep06}.
We claim that the approach presented in this paper has most of the benefits of the genetic
approaches, although the rationale is completely different.

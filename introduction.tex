
\section{Introduction and Motivation}

The design of an embedded system requires different conflicting
objectives (energy consumption, performance, area) to be fulfilled.
The growing demand and reduced time-to-market of these systems are
introducing even more challenges that designers have to
face~\cite{wsts}.
Several factors are involved in this goal and real cases show how
often no analytical results are known to predict the relation between
system configurations and objectives. In such cases a high level
system simulation is the only way to have a picture of how system
parameters impact on the objectives.  A key problem is that the size
of the design space to be simulated grows with the product of
cardinalities of each parameter, resulting in an intractable number
of simulations. 

In these cases a Design Space
Exploration (DSE) strategy is required, i.e. an algorithm that selects
only a tractable subset of all possible configurations.
The final result of a design space exploration is a subset of the
configurations called \emph{Pareto set} (\cite{pareto}, see also
\defR{pareto-set}). The corresponding values of the objectives for
each configuration of the Pareto Set is called \emph{Pareto front}:
it represents the trade-offs between objectives, i.e.  for each point
of the Pareto front there is no other configuration performing better
for all the objectives considered (see \figR{psos}). Of course this is only a first
high-level system simulation, thus these few promising
configurations will need to be further evaluated in a next step of the design
flow using a more accurate low level simulation (e.g. register
transfer level).

%while providing, at the end, configurations that are close to the
%optimum and that are worth evaluating, in a following step of the
%design process, in a more thorough way.


In this work we present PS, a multiobjective design exploration
algorithm that introduces the concept of \emph{interesting} and
\emph{uninteresting regions} of the configuration space.  The main
idea can be simplified as follows: at each iteration of the algorithm,
the configuration space is divided in regions. A score is given to
each region based on the \emph{innovation} provided by its
configurations in the Pareto front, i.e. how different these configurations are, in terms of objectives, with respect to the ones already found previously. The regions with high score are considered the most \emph{interesting} and more simulations are distributed there rather than in the rest of the configuration space.
Several geometrical transformations (splitting, merging etc.) are
involved in the Parameter Space (aka configuration space), hence
the name PS for the algorithm.

From a more abstract but interesting perspective, the proposed
algorithm mimics the attention shift in human beings. For example,
authors in \cite{attention} models human attention as a single
resource with a limited budget that must be distributed among tasks,
preferring the task which the focus is on.  Authors in
\cite{spatial_attention} describes the visual attention as a spotlight
that focuses, each time, in a particular region of the visual space in
order to process it more deeply. Similarly, PS focuses its
``spotlight'' only to the interesting regions and explore them more
thoroughly distributing the simulation budget on them.
% Ispirzione: http://en.wikipedia.org/wiki/Attentional_shift
In other words, with PS we propose a new perspective on the configuration space,
where parameter interdependencies play the fundamental role of making
some regions more capable of generating hardly predictable solutions:
catching this sort of ``chaoticity'' in the parameter space is the
main idea behind the strategy that is being introduced in this work.


The paper is organized as follows. First, \secR{Related-work} places our work in the context of related effort.
Then we give a theoretical formulation of the algorithm. In particular, in \secR{statement_of_the_problem} we provide formal definitions of the concepts and the operations that will be used to describe PS algorithm. In \secR{algorithm} we give the theoretical description of PS. 
Finally in \secR{ee} we show a case
study involving the exploration of the hardware/software parameters of
a Very Long Instruction Word (VLIW) architecture, performing a
qualitative and quantitative comparison of PS against the state-of-art
of multiobjective genetic based approaches.


\section{Related work}
\secL{Related-work}
Many different design space exploration algorithms have been proposed
in literature.  The motivations of an exploration
algorithm are rather heuristic, have some form of arbitrariness and,
to a large extend, intuition lies behind them.

Some exploration strategies assume some kind of knowledge about the
system parameters, their meaning and their impact on design
objectives.  The \emph{Dependency analysis}, proposed in
\cite{givargis_tvlsi02}, is meant to take advantage of the parameter
dependencies. Starting from them, he can construct a ``dependency graph''
and recognize clusters, i.e. subsets of strongly dependency-connected
parameters. Each cluster is exhaustively evaluated and its ``local''
Pareto set is found. Then, all local Pareto sets are merged. In this
way, a series of local exhaustive evaluations are performed instead of
an exhaustive evaluation of all the possible configurations. Some
problems arise: i) In real scenarios, it may be difficult to recognize
really independent clusters of parameters, because there may be
interdependencies among a large number of parameters. This may lead to
the need of an evaluation of almost all the possible configurations.
ii) A designer may not have a precise and complete a-priori picture of
the dependencies among parameters; for this reason the need of
``automated approaches for computing interdependencies'' is declared
in the same paper.  These drawbacks are not present in PS,
since, although it leverages dependencies as in
\cite{givargis_tvlsi02}, it is able to detect them automatically, with
no a-priori knowledge required. Moreover, our algorithm is able to
capture also ``local dependencies", i.e. dependencies that emerge only
among certain ranges of parameters and may not hold when considering
the entire ranges. This cannot be modeled in Dependency Analysis. 

Abraham, Rau and Schreiber proposed in \cite{santosh_hptr00} to decompose
the system under evaluation into components that interact minimally
with each other. Pareto sets for each component are found and, provided
that ``monotonicity'' exists, the complete Pareto set is computed
merging the component Pareto sets. Roughly speaking (see section 4
of the same paper for more details), monotonicity property guarantees
that the best system can be obtained as a composition of the best
components. This approach would perfectly work if all the components
were perfectly isolated, i.e., if there were no dependencies among
them. But real scenarios rarely expose monotonicity property thus resulting in inaccuracies, as stated in the same paper).

Other approaches, as \cite{fornaciari_codes01,palesi_iwsoc02}, are based the concept of \emph{sensitivity analysis}, i.e. measuring of how much the objective varies when varying each parameter.
Parameters are ordered based on their sensitivity. The first parameter (the most sensitive one) is varied, while keeping the other parameters fixed to arbitrary values, and its best value is found. The limited accuracy shown by these approaches can be explained by the limited and rigid exploration of the parameter
space: after fixing the best first parameter value, there are no more chances to consider different values. It is worth noticing that this approach can not capture
``local sensitivity'', i.e. the objectives may be more sensitive to some
ranges of a parameter and less sensitive to other ranges of the same
parameter. Moreover it can not capture ``combined sensitivity'', i.e. the objectives may be
more sensitive to a range of a parameter $p_{1}$, only when other
parameters are within certain ranges, and less sensitive to the same
range of $p_{1}$ when the other parameters are in different ranges. Otherwise, our algorithm can both capture local and combined sensitivity.

\cite{dellnitz2005covering} proposes three different methods for the numerical approximation of the Pareto set. The first two require the knowledge of the gradient of the objective functions, which is usually not available in embedded system design, since the closed form expression of the objective functions is not known a priori. The third one, called ``Sampling algorithm'', does not need this requirement and is the closest to ours, although the mechanism is different. At every iteration a set of regions of configuration space is constructed with the aim to cover the Pareto set. The size of these regions becomes smaller at each iteration so that the covering becomes tighter. A random set of test points are evaluated in each new region; only the regions containing non-dominated points will be considered in the next iteration. Our approach is different. First, in the Sampling algorithm only the selected regions are examined, even though there is no guarantee that all the Pareto points are included in them. As a consequence, the Sampling algorithm may erroneously and prematurely neglect some regions that may possibly contain a non-negligible number of Pareto points. Our algorithm avoids this, since even though a region has not provided non-dominated points, it is not neglected, rather it is simply populated with fewer test points, thus giving the chance to find non-dominated points in them. From this behavior also comes another important difference. The Sampling algorithm of \cite{dellnitz2005covering} keeps reinforcing the analysis of the regions selected in the previous iterations. In other words, it keeps zooming into the regions that have been considered ``interesting'', i.e. rich of non-dominated points, in the previous iterations. The philosophy behind our algorithm is different. A region, which has been considered interesting at a certain iterations, may be not in the future, since it has been thoroughly analyzed. On the other hand, a region, initially uninteresting, may turn out to be interesting in the future, once all the other regions have been already exploited. Therefore, from an iteration to the next, our algorithm zoom in on regions that are currently interesting and zoom out from the uninteresting ones,  spanning in a broader and more dynamical way the configuration space. In addition, in the Sampling region only the number of non-dominated points is accounted to see how interesting a region is. On the contrary, we also consider the \emph{innovation}, as already anticipated in the introduction.



Many studies, as \cite{coello_easmop} and others, focus on genetic approaches to solve multiobjective
optimization problems. Genetic approaches
have many advantages: they provide good accuracy, they are customizable and very general
and require no a-priori knowledge to the designer.  The strong
point of genetic approaches can be summarized as follows: they consist
in an exploration that is sufficiently broad (the mutation avoids being rigidly restricted to limited parts of parameter space), and, at the same time, not too scattered as it is guided by the performances of
the already evaluated configurations. In this work we will compare 
the proposed approach against a well known Multi-Objective Genetic
Algorithm~\cite{knowles_techrep06}.
We claim that the approach presented in this paper has most of the benefits of the genetic
approaches, although the rationale is completely different.

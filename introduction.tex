
\section{Introduction and Motivation}
\comment{Si capisce che noi provide an algorithm?}

\comment{Mettere our contribution?}

The design of an embedded system requires different conflicting
objectives (energy consumption, performance, area) to be fulfilled.
Several factors are involved in this goal and real cases show how
often no analytical results are known to predict the relation between
system configurations and objectives. In such cases a high level
system simulation is often the only way to have a picture of how system
parameters impact on the objectives.  A key problem is that the size
of the design space to be simulated grows with the product of
cardinalities of each parameter, easily resulting in an intractable number
of simulations. In these cases a ``Design Space
Exploration'' strategy is required, i.e. a methodology to evaluate
only a small subset of all possible configurations while maintaining a
good level of efficiency and accuracy~\cite{surviving_soc}.

The final result of a design space exploration is a subset of the
configurations called Pareto Set (\cite{pareto}) (See also Definition
\ref{pers02.def:Pareto-set}). It represents the trade-offs between
objectives, so that for each configuration of the Pareto Set there is no
other configuration performing better for all the objectives
considered. Once these few promising configurations have been obtained, a
subsequent step of the design flow can eventually afford a more
accurate low level simulation.

In this work we present a multiobjective design exploration strategy
that introduces the concept of Parameter Space Representation of
Pareto Set (PS). The main idea is focusing on the ``innovation''
introduced in the Pareto sets and then accordingly distributing the
exploration effort in regions created in the multidimensional space of
system parameters. The aim of the PS approach is to propose a new
perspective on the configuration space, where parameter
interdependencies play the fundamental role of making some regions
more capable of generating hardly predictable solutions: catching this
sort of ``chaoticity'' in the parameter space is the main idea behind
the strategy that is being introduced in this work.

\section{Related work and Contribution}
\label{sec:Related-work}
Many different design space exploration algorithms have been proposed
in literature.  The motivations of an exploration
algorithm are rather heuristic, have some form of arbitrariness and,
to a large extend, intuition lies behind them.

Some exploration strategies assume some kind of knowledge about the
system parameters, their meaning and their impact on design
objectives.  The \emph{Dependency analysis}, proposed in
\cite{givargis_tvlsi02}, is meant to take advantage of the parameter
dependencies. Starting from them, he can construct a ``dependency graph''
and recognize clusters, i.e. subsets of strongly dependency-connected
parameters. Each cluster is exhaustively evaluated and its ``local''
Pareto set is found. Then, all local Pareto sets are merged. In this
way, a series of local exhaustive evaluations are performed instead of
an exhaustive evaluation of all the possible configurations. Some
problems arise: i) In real scenarios, it may be difficult to recognize
really independent clusters of parameters, because there may be
interdependencies among a large number of parameters. This may lead to
the need of an evaluation of almost all the possible configurations.
ii) A designer may not have a precise and complete a-priori picture of
the dependencies among parameters; for this reason the need of
``automated approaches for computing interdependencies'' is declared
in the same paper.  These drawbacks are not present in our algorithm,
since, although it leverages dependencies as in
\cite{givargis_tvlsi02}, it is able to detect them automatically, with
no a-priori knowledge required. Moreover, our algorithm is able to
capture also ``local dependencies", i.e. dependencies that emerge only
among certain ranges of parameters and may not hold when considering
the entires ranges. This cannot be modelled in Dependency Analysis. 

Abraham, Rau and Schreiber proposed in \cite{santosh_hptr00} to decompose
the system under evaluation into components that interact minimally
with each other. Pareto sets for each component are found and, provided
that ``monotonicity'' exists, the complete Pareto set is computed
merging the component Pareto sets. Roughly speaking (see section 4
of the same paper for more details), monotonicity property guarantees
that the best system can be obtained as a composition of the best
components. This approach would perfectly work if all the components
were perfectly isolated, i.e., if there were no dependencies among
them. But real scenarios seldom if ever
expose monotonicity property thus resulting in inaccuracies, as stated in the same paper).

Other approaches, as \cite{fornaciari_codes01,palesi_iwsoc02}, are based the concept of \emph{sensitivity analysis}, i.e. measuring of how much the objective varies when varying each parameter.
Parameters are ordered based on their sensitivity. The first parameter (the most sensitive one) is varied, while keeping the other parameters fixed to arbitrary values, and its best value is found. The limited accuracy shown by these approaches can be explained by the limited and rigid exploration of the parameter
space: after fixing the best first parameter value, there are no more chances to consider different values. It is worth noticing that this approach can not capture
``local sensitivity'', i.e. the objectives may be more sensitive to some
ranges of a parameter and less sensitive to other ranges of the same
parameter. Moreover it can not capture ``combined sensitivity'', i.e. the objectives may be
more sensitive to a range of a parameter $p_{1}$, only when other
parameters are within certain ranges, and less sensitive to the same
range of $p_{1}$ when the other parameters are in different ranges. Otherwise, our algorithm can both capture local and combined sensitivity.


Many studies, as \cite{coello_easmop} and others, focus on genetic approaches to solve multiobjective
optimization problems. Genetic approaches
have many advantages: they provide good accuracy, they are customizable and very general
and require no a-priori knowledge to the designer.  The strong
point of genetic approaches can be summarized saying that they consist
in exploration that is sufficiently broad (the mutation avoids being rigidly restricted to limited parts of parameter space), and, at the same time, not too scattered as it is guided by the performances of
the already evaluated configurations. We claim that the
approach presented in this paper has most of the benefits of the genetic
approaches, although the rationale is completely different.

The paper is organized as follows: in
Section~\ref{sec:statement_of_the_problem} we formally introduce the
main concepts and the definitions required to apply the proposed strategy. Next in
Section~\ref{sec:algorithm} we present the PS algorithm in detail.
Finally in Section~\ref{sec:ee} we show a case study involving the
exploration of the hardware/software parameters of a Very Long
 Instruction Word (VLIW) architecture, performing a qualitative
 and quantitative comparison of
PS agains the state-of-art of multiobjective genetic based approaches.

\section{appendix}
Metto qua tutte le cose che non si possono lasciare nel testo ma che e' un peccato buttare nel dimenticatoio

\begin{definition}[Separation (alternative)] Another definition of separation could be:
	\[
	s\left(\mathbf{o}\rightarrow\mathbf{q}\right)=\max_{i=1,\dots,m}\left|\frac{o_{i}-q_{i}}{o_{i}}\right|
	\]
\end{definition}

\subsection{Parallelism}
While a detailed analysis of the parallel capabilities of the algorithm is not in the scope of this paper, we point out that our algorithm shows parallel capabilities. Following the approach used in the ``Master-Slaves parallel genetic algorithms''~\cite{Cantu-Paz98asurvey,Borkar14parallel}, at each era a master processing node assigns to each slave processing node a set of regions. Each processing node independently performs the analysis of the assigned regions and calculates the innovation score. At the end of each era, the master collects the results from the slaves, decide performs the splitting/merging operations and the next era starts. Another possible approach is closer to the ``Island scheme'' for GA parallelization~\cite{Borkar14parallel}. The configuration space is initially divided in $n$ macro-regions, each one assigned to a different processing node. Then, each processing node performs PS algorithm on the assigned macro-regions, autonomously performing all the steps of the algorithm, including merging and splitting. Note that, while the Island scheme for GA parallelization requires a migration operator, that is not easy to properly tune, we do not have special requirements. On the other hand, one of the macro-regions may begin to provide very low innovation at a certain era. In this case, dedicating to it the same number $K$ of test function as the other macro-regions is inefficient. To solve this problems, periodically, the number $k$ of test functions assigned to each macro-region should be renegotiated, in favor of the macro-regions providing most innovation.

\subsection{Region classification}
Refer to step 7 of the algorithm.
AA: Secondo me e' strano usare $\alpha\cdot is_{av,i-1}$ come soglia. Non si capisce perche' non si usi $is_{av,i}$ ma si usi quello dell'era precedente. Non e' motivata da nessuna parte. Non si capisce perche', in una finale di miss italia, io dovrei conforntare le ragazze di oggi con la ragazza media dell'anno scorso. E' piu' naturale confrontare le ragazze di oggi con la ragazza media di oggi stesso, per vedere chi e' oltre alla media attuale. Che ne dici di cambiare l'espressione in 
			\[
			\mathcal{R}_{i,h}=\left\{ \left.R\in\mathcal{R}_{i}\right|is\left(R\right)>\alpha\cdot is_{av,i}\right\} 
			\]
?

\subsection{Other metrics}
Un'altra interessante metrica potrebbe essere questa: Auhtors of \cite{zitzler_ec00} remark that a desirable property of the Pareto front is the spread of the non-dominated configurations all over the configuration space. In other words, the more similar the non-dominated configuration distribution is to the uniform distribution, the better is the quality of the Pareto front, since we have not been captured by only one possible subset of Pareto configurations but we are able to span all over the configuration space and find configurations whose parameter values differ considerably. Mi aspetto che PS performi bene sotto questo punto di vista. Ovviamente, non so se e' il caso di considerarla ora. Se non e' il caso, puoi spostare questo testo nel file ``appendix.tex'' come possibile spunto futuro. Per quantificare questa cosa, si potrebbe usare come metrica la normalised absolute dispersion error pero' non nello spazio degli obiettivi, ma nello spazio delle configurazioni.

\subsection{Virtual merging}
PROBLEMA: Secondo il punto debole del nostro algoritmo, che fa si che noi ci facciamo sempre fregare da GA e' il merging. Il merging puo' essere effettuato solo tra regioni contigue. Non so come siano andati i run, ma ad intuito, trovare delle no-innovation-regions che sono pure contigue e' proprio una botta di fortuna. Secondo me la maggior parte delle no-innovation-regions sono non contigue e quindi non si mergiano, continuando a restistere imperterritamente col procedere dei run. Risultato: le no-innovation-regions continuano a fregarsi, ad ogni iterazione, piu' run di quanti dovrebbero.

SOLUZIONE: Forse per migliorare l'algoritmo si dovrebbe abbandonare il concetto di merging, visto che mi sa che per ora e' sulla carta, ma durante i run avviene poche volte. In alternativa, si potrebbe usare un fattore di qualita', che simuli l'avvenimento dei merge. Ad ogni regione $R$ si associa un fattore di qualita' $q_R$, di default 0. Quando la regione e' classificata come no-innovation, $q_R$ si decrementa. A ciascuna iterazione, anziche' assegnare ad ogni regione K/N simulazioni, dove N Ã¨ il numero di regioni, ogni regione $R*$ dovrebbe avere il seguente numero di simulazioni
	\begin{align}
    \frac{2^{q_R*} K} { \sum_R  2^{q_R} }
	\end{align}

\subsection{Importance given to the innovation score}
AND: una prova che si potrebbe fare e' modificare leggerment l'innovation score di una regione. Forse questa metrica da troppo peso all'innovation delle configurazioni. Se una regione e' ricca di pareto points, ma essi sono tutti simili (nel senso che le loro immagini nello spazio degli obiettivi sono tutte vicine), dopo un po' di iterazioni l'analisi di quella regione non portera' sufficiente innovazione e la regione sara' abbandonata. Il che e' un peccato, perche' magari quei punti che in questo modo ci stiamo perdendo permetterebbero di fregare, o eguagliare, genetic algo. Questo spiega anche perche' le nostre soluzioni spaziano di piu' lo spazio degli obiettivi e vanno piu' verso gli estremi. Se volessimo rinunciare a spaziare per forza tutto il range degli obiettivi, volendo essere piu' simili e competitivi con genetic algorithm, potremmo modificare il calcolo dell'innovation score (step (4) della sezione 4.2), nel seguente modo:
	\begin{align}
	    is(c^*) = 1 + B\cdot \min[ s(c \rightarrow c^*) | c \in \mathscr{P}_{i-1} ]
	\end{align}
In questo modo, il fattore B permette di pesare quanta importanza dare all'innovativita' del nuovo pareto point. Ad es, per B=0, tutti i nuovi pareto points sono buoni uguali, a prescindere dalla loro innovativita'. Per B=0.2, un nuovo pareto point gia' e' una cosa bella, se poi ha innovativita', meglio ancora, sarebbe la ciliegina sulla torta, ecc .... Si potrebbero fare nuovi run con questo calcolo di innovation score, io li farei per B=1 e B=0.5, per vedere se le cose migliorano.
